---
title: "Separating Spam from Ham"
author: "Solutions by John Bobo based on a problem set from MITâ€™s Analytics Edge MOOC"
date: "May 27, 2016"
output:
    html_document:
        theme: cerulean
        keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits = 3)
```
Nearly every email user has at some point encountered a "spam" email, which is an unsolicited message often advertising a product, containing links to malware, or attempting to scam the recipient. Roughly 80-90% of more than 100 billion emails sent each day are spam emails, most being sent from botnets of malware-infected computers. The remainder of emails are called "ham" emails.

As a result of the huge number of spam emails being sent across the Internet each day, most email providers offer a spam filter that automatically flags likely spam messages and separates them from the ham. Though these filters use a number of techniques (e.g. looking up the sender in a so-called "Blackhole List" that contains IP addresses of likely spammers), most rely heavily on the analysis of the contents of an email via text analytics.

In this homework problem, we will build and evaluate a spam filter using a publicly available dataset first described in the 2006 conference paper "Spam Filtering with Naive Bayes -- Which Naive Bayes?" by V. Metsis, I. Androutsopoulos, and G. Paliouras. The "ham" messages in this dataset come from the inbox of former Enron Managing Director for Research Vincent Kaminski, one of the inboxes in the Enron Corpus. One source of spam messages in this dataset is the SpamAssassin corpus, which contains hand-labeled spam messages contributed by Internet users. The remaining spam was collected by Project Honey Pot, a project that collects spam messages and identifies spammers by publishing email address that humans would know not to contact but that bots might target with spam. The full dataset we will use was constructed as roughly a 75/25 mix of the ham and spam messages.

The dataset contains just two fields:

- **text**: The text of the email.
- **spam**: A binary variable indicating if the email was spam.

***

#### Problem 1.1 - Loading the Dataset

(1 point possible)

Begin by loading the dataset [emails.csv](https://d37djvu3ytnwxt.cloudfront.net/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/emails.csv) into a data frame called emails. Remember to pass the `stringsAsFactors=FALSE` option when loading the data.
```{r}
emails <- read.csv("/Users/johnbobo/analytics_edge/data/emails.csv",
                  stringsAsFactors = FALSE)
```

*How many emails are in the dataset?*
```{r}
answer <- nrow(emails)
```
**Answer:** `r answer`

***

#### Problem 1.2 - Loading the Dataset

(1 point possible)
*How many of the emails are spam?*
```{r}
answer <- sum(emails$spam)
```
**Answer:** `r answer`

***

#### Problem 1.3 - Loading the Dataset

(1 point possible)
*Which word appears at the beginning of every email in the dataset?*

```{r}
substring(emails$text[1],1,7)
```
**Answer:** Subject

***

#### Problem 1.4 - Loading the Dataset

(1 point possible)
*Could a spam classifier potentially benefit from including the frequency of the word that appears in every email?*  

**Answer:** Yes, the number of times the word appears in a single email might help us differentiate spam from ham.  

***

#### Problem 1.5 - Loading the Dataset

(1 point possible)
The nchar() function counts the number of characters in a piece of text. *How many characters are in the longest email in the dataset (where longest is measured in terms of the maximum number of characters)?*
```{r}
answer <- max(nchar(emails$text))
```
**Answer:** `r answer`

***

#### Problem 1.6 - Loading the Dataset

(1 point possible)
*Which row contains the shortest email in the dataset? (Just like in the previous problem, shortest is measured in terms of the fewest number of characters.)*
```{r}
answer <- which.min(nchar(emails$text))
```
**Answer:** `r answer`

***

#### Problem 2.1 - Preparing the Corpus

(2 points possible)
Follow the standard steps to build and pre-process the corpus:

1) Build a new corpus variable called corpus.

2) Using tm_map, convert the text to lowercase.

3) Using tm_map, remove all punctuation from the corpus.

4) Using tm_map, remove all English stopwords from the corpus.

5) Using tm_map, stem the words in the corpus.

6) Build a document term matrix from the corpus, called dtm.

```{r}
library(tm)

vectorToDTM <-function(data){
    corpus = Corpus(VectorSource(data))
    corpus = tm_map(corpus, content_transformer(tolower))
    corpus = tm_map(corpus, PlainTextDocument)
    corpus = tm_map(corpus, removePunctuation)
    corpus = tm_map(corpus, removeWords, stopwords('english'))
    corpus = tm_map(corpus, stemDocument)
    
    dtm = DocumentTermMatrix(corpus)
    return(dtm)
}

dtm <- vectorToDTM(emails$text)
dtm
```

*How many terms are in dtm?*  

**Answer:** `r dtm$ncol`

***

#### Problem 2.2 - Preparing the Corpus

(1 point possible)
To obtain a more reasonable number of terms, limit dtm to contain terms appearing in at least 5% of documents, and store this result as spdtm (don't overwrite dtm, because we will use it in a later step of this homework).
```{r}
spdtm <- removeSparseTerms(dtm, .95)
spdtm
```
*How many terms are in spdtm?*
**Answer:** `r spdtm$ncol`

***

#### Problem 2.3 - Preparing the Corpus

(2 points possible)
Build a data frame called emailsSparse from spdtm, and use the make.names function to make the variable names of emailsSparse valid.
```{r}
emailsSparse <- as.data.frame(as.matrix(spdtm))
colnames(emailsSparse) <- make.names(colnames(emailsSparse))
```

colSums() is an R function that returns the sum of values for each variable in our data frame. Our data frame contains the number of times each word stem (columns) appeared in each email (rows). Therefore, colSums(emailsSparse) returns the number of times a word stem appeared across all the emails in the dataset. *What is the word stem that shows up most frequently across all the emails in the dataset?*
```{r}
answer <- names(which.max(colSums(emailsSparse)))
```
**Answer:** `r answer`

***

#### Problem 2.4 - Preparing the Corpus

(1 point possible)
Add a variable called "spam" to emailsSparse containing the email spam labels. You can do this by copying over the "spam" variable from the original data frame (remember how we did this in the Twitter lecture).
```{r}
emailsSparse$spam <- emails$spam
```

*How many word stems appear at least 5000 times in the ham emails in the dataset?*
```{r}
answer <- sum(colSums(subset(emailsSparse, spam == 0)) > 5000)
```
**Answer:** `r answer`

***

#### Problem 2.5 - Preparing the Corpus

(1 point possible)
*How many word stems appear at least 1000 times in the spam emails in the dataset?*
remember not to count the dependent variable we just added.
```{r}
answer <- sum(colSums(subset(emailsSparse, spam == 1)) > 1000) - 
                                                (sum(emailsSparse$spam) > 1000)
```
**Answer:** `r answer`

***

#### Problem 2.6 - Preparing the Corpus

(1 point possible)
The lists of most common words are significantly different between the spam and ham emails. *What does this likely imply?*  

**Answer:** The frequencies of these most common words are likely to help differentiate between spam and ham.

***

#### Problem 2.7 - Preparing the Corpus

(1 point possible)
Several of the most common word stems from the ham documents, such as "enron", "hou" (short for Houston), "vinc" (the word stem of "Vince") and "kaminski", are likely specific to Vincent Kaminski's inbox. *What does this mean about the applicability of the text analytics models we will train for the spam filtering problem?*  

**Answer:** The models we build are personalized, and would need to be further tested before being used as a spam filter for another person.

***