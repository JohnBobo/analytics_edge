---
title: "Popularity of Music Records"
author: "By John Bobo based on a problem set from MITâ€™s Analytics Edge MOOC"
date: "May 13, 2016"
output:
    html_document:
        theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=4)
```
The music industry has a well-developed market with a global annual revenue aroundÂ $15 billion. The recording industry is highly competitive and is dominated by three big production companies which make up nearly 82% of the total annual album sales.Â   
  
Artists are at the core of the music industry and record labels provide them with the necessary resources to sell their music on a large scale. A record label incurs numerous costs (studio recording, marketing, distribution, and touring) in exchange for a percentage of the profits from album sales, singles and concert tickets.  
  
Unfortunately, the success of an artist's release is highly uncertain: a single may be extremely popular, resulting in widespread radio play and digital downloads, while another single may turn out quite unpopular, and therefore unprofitable.Â   
  
Knowing the competitive nature of the recording industry, record labels face the fundamental decision problem of which musical releases to support to maximize their financial success.Â   
  
How can we use analytics to predict the popularity of a song?Â In this assignment, we challenge ourselves to predict whether a song will reach a spot in the Top 10 of the Billboard Hot 100 Chart.

Taking an analytics approach, we aim to use information about a song's properties to predict its popularity. The datasetÂ [songs.csv](https://d37djvu3ytnwxt.cloudfront.net/asset-v1:MITx+15.071x_3+1T2016+type@asset+block/songs.csv)Â consists of all songsÂ which made it to the Top 10 of the Billboard Hot 100 Chart from 1990-2010 plus a sample of additional songs that didn't make the Top 10.Â This data comes from three sources:[Wikipedia](http://en.wikipedia.org/wiki/Billboard_Hot_100),Â [Billboard.com](http://www.billboard.com/), andÂ [EchoNest](http://echonest.com/).

The variables included in the dataset either describe the artist or the song, or they are associated with the following song attributes: time signature, loudness, key, pitch, tempo, and timbre.

Here's a detailed description of the variables:

- **year**Â = the year the song was released
- **songtitle**Â = the title of the song
- **artistname**Â = the name of the artist of the song
- **songID**Â andÂ **artistID**Â = identifying variables for the song and artist
- **timesignature**Â andÂ **timesignature_confidence**Â = a variable estimating the time signature of the song, and the confidence in the estimate
- Â **loudness**Â = a continuous variable indicating the average amplitude of the audio in decibels
- Â **tempo**Â andÂ **tempo_confidence**Â = a variable indicating the estimated beats per minute of the song, and the confidence in the estimate
- Â **key**Â andÂ **key_confidence**Â = a variable with twelve levels indicating the estimated key of the song (C, C#, . . ., B), and the confidence in the estimate
- Â **energy**Â = a variable that represents the overall acoustic energy of the song, using a mix of features such as loudness
- **pitch**Â = a continuous variable that indicates the pitch of the song
- Â **timbre_0_min**,Â **timbre_0_max**,Â **timbre_1_min**,Â **timbre_1_max**, . . . ,Â **timbre_11_min**, andÂ **timbre_11_max**Â = variables that indicate the minimum/maximum values over all segments for each of the twelve values in the timbre vector (resulting in 24 continuous variables)
- **Top10**Â = a binary variable indicating whether or not the song made it to the Top 10 of the BillboardÂ Hot 100Â Chart (1 if it was in the top 10, and 0 if it was not)

***

#### Problem 1.1 - Understanding the Data


Use the read.csv function to load the dataset "songs.csv" into R.
```{r}
songs <- read.csv("/Users/johnbobo/analytics_edge/data/songs.csv")
```

**How many observations (songs) are from the year 2010?**
```{r}
nrow(songs[songs$year==2010,])
```
**Answer:** `r nrow(songs[songs$year==2010,])`

***

#### Problem 1.2 - Understanding the Data


**How many songs does the dataset include for which the artist name is "Michael Jackson"?**
```{r}
mj <- subset(songs, artistname == "Michael Jackson")
nrow(mj)
```
**Answer:** `r nrow(mj)`

***

#### Problem 1.3 - Understanding the Data


**Which songs by Michael Jackson made it to the Top 10?**
```{r}
subset(mj, Top10 == 1)$songtitle
```
**Answer:** You Rock My World, You Are Not Alone, Black or White, Remember the Time, In The Closet.

***

#### Problem 1.4 - Understanding the Data


The variable corresponding to the estimated time signature (timesignature) is discrete, meaning that it only takes integer values (0, 1, 2, 3, . . . ). **What are the values of this variable that occur in our dataset?**
```{r}
unique(songs$timesignature)
```
**Answer:** 3, 4, 5, 7, 1, 0

**Which timesignature value is the most frequent among songs in our dataset?**  
```{r}
sort(table(songs$timesignature), decreasing=TRUE)[1]
```

**Answer**: 4

***

#### Problem 1.5 - Understanding the Data


**What is a song with the highest tempo**
```{r}
songs$songtitle[which.max(songs$tempo)]
```
**Answer:** Wanna Be Startin' Somethin'

***

#### Problem 2.1 - Creating Our Prediction Model


We wish to predict whether or not a song will make it to the Top 10. To do this, first use the subset function to split the data into a training set "songsTrain" consisting of all the observations up to and including 2009 song releases, and a testing set "songsTest", consisting of the 2010 song releases.
```{r}
songsTrain <- subset(songs, year <= 2009)
songsTest <- subset(songs, year == 2010)
```

**How many observations (songs) are in the training set?**
```{r}
nrow(songsTrain)
```
**Answer:** `r nrow(songsTrain)`

***

#### Problem 2.2 - Creating our Prediction Model


In this problem, our outcome variable is "Top10" - we are trying to predict whether or not a song will make it to the Top 10 of the Billboard Hot 100 Chart. Since the outcome variable is binary, we will build a logistic regression model. We'll start by using all song attributes as our independent variables, which we'll call Model 1.

We will only use the variables in our dataset that describe the numerical attributes of the song in our logistic regression model. So we won't use the variables "year", "songtitle", "artistname", "songID" or "artistID".
```{r}
non_vars <- c("year", "songtitle", "artistname", "songID", "artistID")
songsTrain <- songsTrain[,!(names(songsTrain) %in% non_vars)]
songsTest <- songsTest[,!(names(songsTest) %in% non_vars)]
```
Now, use the glm function to build a logistic regression model to predict Top10 using all of the other variables as the independent variables. You should use SongsTrain to build the model.
```{r}
songs_log <- glm(Top10 ~ ., data=songsTrain, family=binomial)
summary(songs_log)
```

**Looking at the summary of your model, what is the value of the Akaike Information Criterion (AIC)?**  
**Answer:** `r summary(songs_log)$aic`

***

#### Problem 2.3 - Creating Our Prediction Model


Let's now think about the variables in our dataset related to the confidence of the time signature, key and tempo (timesignature_confidence, key_confidence, and tempo_confidence). Our model seems to indicate that these confidence variables are significant (rather than the variables timesignature, key and tempo themselves). **What does the model suggest?**  
**Answer:** The higher our confidence about time signature, key and tempo, the more likely the song is to be in the Top 10.

***

#### Problem 2.4 - Creating Our Prediction Model


In general, if the confidence is low for the time signature, tempo, and key, then the song is more likely to be complex. **What does our model suggest in terms of complexity?**  
**Answer:** Mainstream listeners tend to prefer less complex songs.

***

#### Problem 2.5 - Creating Our Prediction Model


Songs with heavier instrumentation tend to be louder (have higher values in the variable "loudness") and more energetic (have higher values in the variable "energy").

**By inspecting the coefficient of the variable "loudness", what does our model suggest?**  
**Answer:** The coefficient estimate for loudness is positive, implying mainstream listeners prefer louder songs, which are those with heavier instrumentation. However, the coefficient estimate for energy is negative, meaning that mainstream listeners prefer songs that are less energetic, which are those with light instrumentation. These coefficients lead us to different conclusions!

***

#### Problem 3.1 - Beware of Multicollinearity Issues!


**What is the correlation between the variables "loudness" and "energy" in the training set?**  
```{r}
cor(songsTrain$loudness, songsTrain$energy)
```
**Answer:** `r cor(songsTrain$loudness, songsTrain$energy)` --- This means we have an issue with multicollinearity and should drop one of these variables in our model.  

***

#### Problem 3.2 - Beware of Multicollinearity Issues!


Create Model 2, which is Model 1 without the independent variable "loudness".
```{r}
songs_log_2 <- glm(Top10 ~ . -loudness, data=songsTrain, family=binomial)
summary(songs_log_2)
```
**Inspect the coefficient of the variable "energy". What do you observe?**  
**Answer:** The coefficient estimate for energy is positive implying mainstream listeners prefer heavier instrumentation. However, the variable energy is not significant in this model.

***

#### Problem 3.3 - Beware of Multicollinearity Issues!


Now, create Model 3, which should be exactly like Model 1, but without the variable "energy".
```{r}
songs_log_3 <- glm(Top10 ~ . -energy, data=songsTrain, family=binomial)
summary(songs_log_3)
```

Look at the summary of Model 3 and inspect the coefficient of the variable "loudness". Remembering that higher loudness and energy both occur in songs with heavier instrumentation, **do we make the same observation about the popularity of heavy instrumentation as we did with Model 2?**  
**Answer:** Yes because the coefficient estimate is positive for loudness. Additionally this variable is significant in Model 3.

***

#### Problem 4.1 - Validating Our Model


Make predictions on the test set using Model 3. What is the accuracy of Model 3 on the test set, using a threshold of 0.45? **(Compute the accuracy as a number between 0 and 1.)**
```{r}
predTest = predict(songs_log_3, newdata=songsTest, type="response")
table(songsTest$Top10, predTest >= 0.45)
```
**Answer:** `r (309 + 19)/(nrow(songsTest))`

***

#### Problem 4.2 - Validating Our Model


Let's check if there's any incremental benefit in using Model 3 instead of a baseline model. Given the difficulty of guessing which song is going to be a hit, an easier model would be to pick the most frequent outcome (a song is not a Top 10 hit) for all songs. What would the accuracy of the baseline model be on the test set? (Give your answer as a number between 0 and 1.)
```{r}
table(songsTest$Top10)
```
**Answer:** `r 314/(314+59)`

***

#### Problem 4.3 - Validating Our Model


It seems that Model 3 gives us a small improvement over the baseline model. Still, does it create an edge?

Let's view the two models from an investment perspective. A production company is interested in investing in songs that are highly likely to make it to the Top 10. The company's objective is to minimize its risk of financial losses attributed to investing in songs that end up unpopular.

A competitive edge can therefore be achieved if we can provide the production company a list of songs that are highly likely to end up in the Top 10. We note that the baseline model does not prove useful, as it simply does not label any song as a hit. Let us see what our model has to offer.

**How many songs does Model 3 correctly predict as Top 10 hits in 2010 (remember that all songs in 2010 went into our test set), using a threshold of 0.45?**    
**Answer:** 19

**How many non-hit songs does Model 3 predict will be Top 10 hits (again, looking at the test set), using a threshold of 0.45?**  
**Answer:** 5

***

#### Problem 4.4 - Validating Our Model


**What is the sensitivity of Model 3 on the test set, using a threshold of 0.45?**  
**Answer:** `r 19/(19+40)`

**What is the specificity of Model 3 on the test set, using a threshold of 0.45?**  
**Answer:** `r 309/(309+5)`

***

#### Problem 4.5 - Validating Our Model


**What conclusions can you make about our model?**  
**Answer:** Model 3 favors specificity over sensitivity.  It gives us conservative predictions and rarely predicts a song will make it to the Top 10. How ever when it does predict a song being in the Top 10, we can be pretty confident it will make it to the Top 10.